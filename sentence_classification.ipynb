{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification with Keras and ELMO Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:06:23.489220Z",
     "start_time": "2019-05-24T19:06:09.998553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0524 14:06:20.805979 31728 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Base Imports \n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "# Import our dependencies\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "from keras import backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.engine import Layer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import multi_gpu_model\n",
    "# Initialize session and make sure tensorflow doesn't hog all of the gpu for itself by setting the allow growth config\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:06:23.502182Z",
     "start_time": "2019-05-24T19:06:23.499189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore all this it's code to run benchmarks\n",
    "# with open(\"H:/NLP/intent/NLU-Evaluation-Corpora/Export.json\",encoding=\"latin-1\") as f:\n",
    "#     a = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:06:23.517141Z",
     "start_time": "2019-05-24T19:06:23.512155Z"
    }
   },
   "outputs": [],
   "source": [
    "# dft = pd.DataFrame(columns=[\"text\",\"intents\"])\n",
    "# for i in a[\"resource\"][\"intents\"]:\n",
    "#     intnt =  i[\"name\"]\n",
    "#     print(len(i[\"sampleUtterances\"]))\n",
    "#     for j in i[\"sampleUtterances\"]:\n",
    "#         dft = dft.append({'text': j,'intents':intnt}, ignore_index=True)\n",
    "# dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T16:22:05.747390Z",
     "start_time": "2019-04-01T16:22:05.745396Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_json(json.dumps(a[\"sentences\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T16:22:06.157224Z",
     "start_time": "2019-04-01T16:22:06.155205Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:06:23.601947Z",
     "start_time": "2019-05-24T19:06:23.528113Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read training data -- replace path with current local path for your files\n",
    "df1 = pd.read_csv(\"emotion.csv\",names=[\"text\",\"intents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:08:22.940400Z",
     "start_time": "2019-05-24T19:08:22.695809Z"
    }
   },
   "outputs": [],
   "source": [
    "#Multiprocessing - Code to parallelize data preprocessing\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "# df1[\"a\"] = pd.to_numeric(df[\"a\"])\n",
    "def label_data(row):\n",
    "    import string\n",
    "    #Do any row transformations you need to do in this function. \n",
    "    return row[\"text\"].lower().translate(str.maketrans('', '', string.punctuation))\n",
    "#SPEED BOOST \n",
    "data = df1 #<--Input Dataframe\n",
    "ddata = dd.from_pandas(data, npartitions=30)#You might want to reduce the number of partitions based on availiable threads I had 16 cores\n",
    "df1['text'] = ddata.map_partitions(lambda df1: df1.apply((lambda row: label_data(row)), axis=1)).compute(scheduler='threads') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:08:25.368171Z",
     "start_time": "2019-05-24T19:08:25.365141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'intents'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:08:26.101721Z",
     "start_time": "2019-05-24T19:08:26.093741Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:08:33.992643Z",
     "start_time": "2019-05-24T19:08:33.989618Z"
    }
   },
   "outputs": [],
   "source": [
    "#15884 sentences classified as 7 intents\n",
    "df1=df1[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:08:36.956991Z",
     "start_time": "2019-05-24T19:08:36.948015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>i felt something akin to shame after a heavy n...</td>\n",
       "      <td>shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528</th>\n",
       "      <td>when my cousin passed away</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6305</th>\n",
       "      <td>my lovely girlfriend doublecrossed me and so t...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>i was disgusted when my brother was arrested b...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>i was waiting to receive the participation on ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  intents\n",
       "2157  i felt something akin to shame after a heavy n...    shame\n",
       "6528                         when my cousin passed away  sadness\n",
       "6305  my lovely girlfriend doublecrossed me and so t...    anger\n",
       "6307  i was disgusted when my brother was arrested b...  disgust\n",
       "2561  i was waiting to receive the participation on ...    anger"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:08:59.399483Z",
     "start_time": "2019-05-24T19:08:59.393499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom layer that allows us to update weights (lambda layers do not have trainable parameters!)\n",
    "# Essentially the advantage of doing this is that we can have elmo embeddings in our network that can be trained \n",
    "# Other methods only allow you load in pre trained elmo embeddings\n",
    "\n",
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024\n",
    "        self.trainable=True\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "\n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                      as_dict=True,\n",
    "                      signature='default',\n",
    "                      )['default']\n",
    "        return result\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return K.not_equal(inputs, '--PAD--')\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:09:02.450158Z",
     "start_time": "2019-05-24T19:09:02.443182Z"
    }
   },
   "outputs": [],
   "source": [
    "# One hot encode the intents\n",
    "df2 = pd.get_dummies(df1,columns=['intents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:09:06.037297Z",
     "start_time": "2019-05-24T19:09:06.033306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle Dataset\n",
    "df2 =df2.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:09:08.655707Z",
     "start_time": "2019-05-24T19:09:08.646706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intents_anger</th>\n",
       "      <th>intents_disgust</th>\n",
       "      <th>intents_fear</th>\n",
       "      <th>intents_guilt</th>\n",
       "      <th>intents_joy</th>\n",
       "      <th>intents_sadness</th>\n",
       "      <th>intents_shame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6486</th>\n",
       "      <td>i reached the bus stop and realized that i had...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>i was trying to have sex with my best friends ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>the way my husband and his family treated me  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>when i was in the 12th standard i could not sp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4902</th>\n",
       "      <td>my best friend started moving out with my boyf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  intents_anger  \\\n",
       "6486  i reached the bus stop and realized that i had...              1   \n",
       "5352  i was trying to have sex with my best friends ...              0   \n",
       "4836  the way my husband and his family treated me  ...              0   \n",
       "7011  when i was in the 12th standard i could not sp...              0   \n",
       "4902  my best friend started moving out with my boyf...              1   \n",
       "\n",
       "      intents_disgust  intents_fear  intents_guilt  intents_joy  \\\n",
       "6486                0             0              0            0   \n",
       "5352                0             0              1            0   \n",
       "4836                1             0              0            0   \n",
       "7011                0             0              0            0   \n",
       "4902                0             0              0            0   \n",
       "\n",
       "      intents_sadness  intents_shame  \n",
       "6486                0              0  \n",
       "5352                0              0  \n",
       "4836                0              0  \n",
       "7011                0              1  \n",
       "4902                0              0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:09:11.540286Z",
     "start_time": "2019-05-24T19:09:11.536296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to build model\n",
    "# Can be tweaked based on size of dataset to prevent overfitting\n",
    "# Change pred layer to match the size of your intent set- currenty set to 7\n",
    "def build_model(): \n",
    "  input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
    "  embedding = ElmoEmbeddingLayer()(input_text)\n",
    "  dense1 = layers.Dense(1024, activation='relu')(embedding)\n",
    "  dense2 = layers.Dense(256,activation=\"relu\")(dense1)\n",
    "  pred = layers.Dense(7, activation='softmax')(dense2)\n",
    "\n",
    "  model = Model(inputs=[input_text], outputs=pred)\n",
    "#   model = multi_gpu_model(base_model, gpus=4)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "  \n",
    "  return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:09:12.988971Z",
     "start_time": "2019-05-24T19:09:12.972984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing to get the data in a suitable shape for elmo embedding\n",
    "train_text = df2['text'].tolist()\n",
    "train_text = [' '.join(t.split()) for t in train_text]\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:09:14.227955Z",
     "start_time": "2019-05-24T19:09:14.222996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'intents_anger', 'intents_disgust', 'intents_fear',\n",
       "       'intents_guilt', 'intents_joy', 'intents_sadness', 'intents_shame'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:09:15.079224Z",
     "start_time": "2019-05-24T19:09:15.071214Z"
    }
   },
   "outputs": [],
   "source": [
    "# To features and targets\n",
    "x = df2[[\"text\"]]\n",
    "y= df2[['intents_anger', 'intents_disgust', 'intents_fear',\n",
    "       'intents_guilt', 'intents_joy', 'intents_sadness', 'intents_shame']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:09:15.874304Z",
     "start_time": "2019-05-24T19:09:15.868320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test - Train split\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.15, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:45:45.773309Z",
     "start_time": "2019-03-25T22:45:45.769319Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T19:11:30.524882Z",
     "start_time": "2019-05-24T19:09:19.501317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mehul.kumar\\AppData\\Local\\Continuum\\anaconda3\\envs\\local\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0524 14:11:29.388374 31728 deprecation.py:323] From C:\\Users\\mehul.kumar\\AppData\\Local\\Continuum\\anaconda3\\envs\\local\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 14:11:30.387258 31728 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "elmo_embedding_layer_1 (Elmo (None, 1024)              4         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 1,313,803\n",
      "Trainable params: 1,313,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T19:09:22.561Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mehul.kumar\\AppData\\Local\\Continuum\\anaconda3\\envs\\local\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0524 14:11:30.704404 31728 deprecation.py:323] From C:\\Users\\mehul.kumar\\AppData\\Local\\Continuum\\anaconda3\\envs\\local\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6401 samples, validate on 1130 samples\n",
      "Epoch 1/10\n",
      "6401/6401 [==============================] - 313s 49ms/step - loss: 1.5924 - acc: 0.3973 - val_loss: 1.3336 - val_acc: 0.4885\n",
      "Epoch 2/10\n",
      "6401/6401 [==============================] - 123s 19ms/step - loss: 1.2914 - acc: 0.5254 - val_loss: 1.3427 - val_acc: 0.4912\n",
      "Epoch 3/10\n",
      "6401/6401 [==============================] - 110s 17ms/step - loss: 1.1614 - acc: 0.5748 - val_loss: 1.2490 - val_acc: 0.5451\n",
      "Epoch 4/10\n",
      "6401/6401 [==============================] - 119s 19ms/step - loss: 1.0722 - acc: 0.6083 - val_loss: 1.1781 - val_acc: 0.5761\n",
      "Epoch 5/10\n",
      "6401/6401 [==============================] - 114s 18ms/step - loss: 0.9773 - acc: 0.6415 - val_loss: 1.1622 - val_acc: 0.5956\n",
      "Epoch 6/10\n",
      "6401/6401 [==============================] - 112s 17ms/step - loss: 0.9033 - acc: 0.6765 - val_loss: 1.1825 - val_acc: 0.6018\n",
      "Epoch 7/10\n",
      "6401/6401 [==============================] - 112s 17ms/step - loss: 0.8048 - acc: 0.7107 - val_loss: 1.1931 - val_acc: 0.5708\n",
      "Epoch 8/10\n",
      "4576/6401 [====================>.........] - ETA: 30s - loss: 0.7409 - acc: 0.7375"
     ]
    }
   ],
   "source": [
    "# IMPORTANT : Change batch size to match how much memmory you have left on your GPU \n",
    "# I reccomend starting small and looking at how much of your GPU memmory is being used and scaling up from there\n",
    "# Change epochs based on size of dataset...Early stopping can be incorporated if you dont want to do that\n",
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=10,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T19:09:30.922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save all our hard work\n",
    "model.save('emot.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions on test data and post processing to make sure it can be used for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T16:31:33.842835Z",
     "start_time": "2019-04-01T16:31:28.073259Z"
    }
   },
   "outputs": [],
   "source": [
    "a = model.predict(x_test)\n",
    "pred = np.argmax(a,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T16:31:33.902673Z",
     "start_time": "2019-04-01T16:31:33.898683Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test2 = np.argmax(y_test.values.astype(np.float32),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T16:31:33.959521Z",
     "start_time": "2019-04-01T16:31:33.956528Z"
    }
   },
   "outputs": [],
   "source": [
    "strList = ['intents_anger', 'intents_disgust', 'intents_fear',\n",
    "       'intents_guilt', 'intents_joy', 'intents_sadness', 'intents_shame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T16:31:34.014374Z",
     "start_time": "2019-04-01T16:31:34.011382Z"
    }
   },
   "outputs": [],
   "source": [
    "result = zip(a[0], strList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T16:31:34.070235Z",
     "start_time": "2019-04-01T16:31:34.066245Z"
    }
   },
   "outputs": [],
   "source": [
    "set(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T16:31:45.055664Z",
     "start_time": "2019-04-01T16:31:44.777413Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "cm = confusion_matrix(y_test2, pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure()\n",
    "intent = encode_text_index(df1,'intents')\n",
    "plot_confusion_matrix(cm, intent)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, intent, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T16:33:44.835671Z",
     "start_time": "2019-03-21T16:33:44.832679Z"
    }
   },
   "source": [
    "### Looks like it does a good job but lets try it out by ourselves to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T20:33:08.262835Z",
     "start_time": "2019-04-01T20:33:07.696350Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights('emo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T23:05:46.536073Z",
     "start_time": "2019-04-01T23:05:31.479611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a wonderful day to be at the park\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(3.561739e-12, 'intents_anger'),\n",
       " (8.854974e-12, 'intents_guilt'),\n",
       " (2.7508438e-09, 'intents_shame'),\n",
       " (4.4459987e-09, 'intents_disgust'),\n",
       " (3.0533151e-06, 'intents_fear'),\n",
       " (3.5546036e-06, 'intents_sadness'),\n",
       " (0.9999933, 'intents_joy')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict(np.array([input()]))\n",
    "strList = ['intents_anger', 'intents_disgust', 'intents_fear',\n",
    "       'intents_guilt', 'intents_joy', 'intents_sadness', 'intents_shame']\n",
    "result = zip(res[0], strList)\n",
    "set(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T20:23:58.530507Z",
     "start_time": "2019-04-01T20:23:58.526519Z"
    }
   },
   "outputs": [],
   "source": [
    "#predicted answers are at the bottom with highest probabiltiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T20:22:32.761130Z",
     "start_time": "2019-04-01T20:22:32.757140Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T22:38:33.287254Z",
     "start_time": "2019-04-10T22:38:30.295814Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T22:45:30.527465Z",
     "start_time": "2019-04-10T22:45:30.523477Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T22:45:31.049200Z",
     "start_time": "2019-04-10T22:45:31.045211Z"
    }
   },
   "outputs": [],
   "source": [
    "# CC coordinating conjunction\n",
    "# CD cardinal digit\n",
    "# DT determiner\n",
    "# EX existential there (like: “there is” … think of it like “there exists”)\n",
    "# FW foreign word\n",
    "# IN preposition/subordinating conjunction\n",
    "# JJ adjective ‘big’\n",
    "# JJR adjective, comparative ‘bigger’\n",
    "# JJS adjective, superlative ‘biggest’\n",
    "# LS list marker 1)\n",
    "# MD modal could, will\n",
    "# NN noun, singular ‘desk’\n",
    "# NNS noun plural ‘desks’\n",
    "# NNP proper noun, singular ‘Harrison’\n",
    "# NNPS proper noun, plural ‘Americans’\n",
    "# PDT predeterminer ‘all the kids’\n",
    "# POS possessive ending parent’s\n",
    "# PRP personal pronoun I, he, she\n",
    "# PRP$ possessive pronoun my, his, hers\n",
    "# RB adverb very, silently,\n",
    "# RBR adverb, comparative better\n",
    "# RBS adverb, superlative best\n",
    "# RP particle give up\n",
    "# TO, to go ‘to’ the store.\n",
    "# UH interjection, errrrrrrrm\n",
    "# VB verb, base form take\n",
    "# VBD verb, past tense took\n",
    "# VBG verb, gerund/present participle taking\n",
    "# VBN verb, past participle taken\n",
    "# VBP verb, sing. present, non-3d take\n",
    "# VBZ verb, 3rd person sing. present takes\n",
    "# WDT wh-determiner which\n",
    "# WP wh-pronoun who, what\n",
    "# WP$ possessive wh-pronoun whose\n",
    "# WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T22:45:31.596065Z",
     "start_time": "2019-04-10T22:45:31.591079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke \n"
     ]
    }
   ],
   "source": [
    "topic = \"\"\n",
    "for i in a:\n",
    "    if i[1][0] == \"N\":\n",
    "        topic += (i[0]+\" \")\n",
    "        continue\n",
    "    else:\n",
    "        if topic != \"\":\n",
    "            print(topic)\n",
    "            topic = \"\"\n",
    "            continue\n",
    "if topic != \"\":\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T22:45:34.948955Z",
     "start_time": "2019-04-10T22:45:34.944965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tell', 'VB'), ('me', 'PRP'), ('a', 'DT'), ('russian', 'JJ'), ('joke', 'NN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:11:56.356934Z",
     "start_time": "2019-03-29T00:11:54.698454Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = input(\"Sentence in lower case : \")\n",
    "final_sentence = \"\"\n",
    "for i in enumerate(sentence.split()):\n",
    "    j = sentence.split()\n",
    "    j1= sentence.split()\n",
    "    bef = pos_tag(sentence.split())\n",
    "    j[i[0]] = j[i[0]].capitalize()\n",
    "    sent = ' '.join(word for word in j)\n",
    "    aft = pos_tag(word_tokenize(sent))\n",
    "    #print(bef[i[0]][1][0])\n",
    "    if bef[i[0]][1] != aft[i[0]][1] or bef[i[0]][1][0]==\"N\":\n",
    "        final_sentence += (j[i[0]]+\" \")\n",
    "        #sentence[i[0]] = j[i[0]]\n",
    "    else:\n",
    "        final_sentence += (j1[i[0]]+\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:11:57.107791Z",
     "start_time": "2019-03-29T00:11:57.104783Z"
    }
   },
   "outputs": [],
   "source": [
    "final_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:11:59.985531Z",
     "start_time": "2019-03-29T00:11:59.980518Z"
    }
   },
   "outputs": [],
   "source": [
    "a = pos_tag(word_tokenize(final_sentence))\n",
    "topic = \"\"\n",
    "for i in a:\n",
    "    if i[1][0] == \"N\":\n",
    "        topic += (i[0]+\" \")\n",
    "        continue\n",
    "    else:\n",
    "        if topic != \"\":\n",
    "            print(topic)\n",
    "            topic = \"\"\n",
    "            continue\n",
    "if topic != \"\":\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-24T19:10:35.089Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_tag(word_tokenize(\"Yan Goodfellow Works for google brain\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:local]",
   "language": "python",
   "name": "conda-env-local-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
